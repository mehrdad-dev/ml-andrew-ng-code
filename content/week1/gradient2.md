---
title: "گرادیان کاهشی قسمت دوم"
date: 2020-09-09T16:31:43+04:30
draft: false
weight : 90
---

![image36.png](../images/image36.png?width=30pc)

در قسمت قبل گرادیان کاهشی را به این صورت معرفی
کردیم، در این قسمت می‌خواهیم به توضیح آلفا و 
عبارت مشتق بپردازیم.
اما برای برای درک بهتر می‌خواهیم با یک مثال ساده تر 
تابعی با یک پارامتر را مینیمم کنیم،  یعنی فرض
می‌کنیم تابع هزینه $J$ فقط یک پارامتر دارد.

تصور کنید تابع $J$ زیر 
را با پارامتر $\theta_1$ در این
نقطه داریم، و از این نقطه
کارمان را شروع می‌کنیم.

![image42.png](../images/image42.png?width=15pc)

کاری که عبارت مشتق می‌کند این است که تانژانت این
نقطه را می‌گیرد، مثل این خط قرمز که با تابع مماس
است.
پس از این عبارت شیب خط را به دست می‌آوریم و 
می‌بینیم که در اینجا شیب خط قرمز ما مثبت است،
پس متوجه شدیم که در
این شکل حاصل مشتق
مثبت است، همچنین
نرخ یادگیری نیز همیشه
عددی مثبت است.

بنابراین تغییر $\theta$ طبق فرمول به این صورت است:
$$ \theta_1 := \theta_1 - \alpha \text{ } \cdot \text{ (positive number)} $$

بنابراین داریم $\theta$ منهای مقداری مثبت که این
باعث می‌شود $\theta$ ما کاهش یابد و به سمت چپ برود!

**همان چیزی که می‌خواهیم، نزدیک شدن به مینیمم!**

و اگر طبق مثال قبل از این 
نقطه جدید شروع کنیم
شیب خط ما منفی خواهد
شد، یعنی مشتق منفی!

![image43.png](../images/image43.png?width=15pc)

بنابراین تغییر تتا طبق فرمول به این صورت است:
$$ \theta_1 := \theta_1 - \alpha \text{ } \cdot \text{ (negative number)} $$

می‌بینیم که داریم تتا یک منهای مقداری منفی که این
باعث می‌شود تتا ما افزایش یابد و به سمت راست برود!

![image44.jpg](../images/image44.jpg?width=30pc)

در شکل سمت چپ مقدار آلفا بسیار کوچک است که
باعث می‌شود گرادیان کاهشی خیلی کند تر به مینیمم
برسد یعنی نیاز داریم قدم های بیشتری به پایین برداریم.

اما در شکل راست آلفا بسیار بزرگ تر است که باعث 
شده گرادیان کاهشی هیچ وقت به مینیمم نرسد.
یعنی گرادیان کاهشی ما همگرا نیست بلکه واگرا است!

**حالا شما جواب بدید!**

چه اتفاقی می‌افتد اگر در شکل زیر پارامتر $\theta_1$ در نقطه 
مینیمم باشد؟!

![image45.png](../images/image45.png?width=20pc)


{{%expand "برای دیدن پاسخ کلیک کن" %}}
اگر تصور کنیم که تتا در این مینیمم موضعی است، و ما می‌دانیم که عبارت مشتق ما در این حالت 0 است، و در واقع داریم:

$$ \theta_1 := \theta_1 - \alpha \text{ } \cdot 0$$

و این به این معنی است که اگر در مینیمم موضعی باشیم، مقدار
$\theta_1$ بدون تغییر باقی می‌ماند!

{{% /expand%}}

گرادیان نزولی به 
<span class="top-dict" data-tipso="local minimum">مینیمم موضعی</span> 
ختم می‌شود حتی 
زمانی که نرخ یادگیری یا
همان آلفا ثابت باشد!

![image46.png](../images/image46.png?width=20pc)

زیرا در هر بار انجام الگوریتم شیب خط حاصل از عبارت
مشتق ملایم تر از حالت دفعه قبلش است و همینطور
که به مینیمم نزدیک تر می‌شویم، مشتق نیز به صفر
میل می‌کند.
بنابراین هر بار مشتق کوچک تر می‌شود و این باعث
می‌شود قدم ها هر بار کوچک تر و کوچک تر شوند.
به این خاطر نیازی نیست در طول زمان مقدار آلفا را
کاهش دهیم!
