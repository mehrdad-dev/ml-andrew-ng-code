---
title: "کاهش نگاشت و موازی سازی داده"
date: 2020-11-29T13:40:01+03:30
draft: false
weight: 50
---

ما می‌توانیم گرادیان کاهشی batch را تقسیم کنیم و تابع هزینه را برای زیر مجموعه ای از داده ها به ماشین های مختلف ارسال کنیم تا بتوانیم الگوریتم خود را به صورت موازی آموزش دهیم.

شما می‌توانید مجموعه آموزشی خود را به تعداد z زیرمجموعه  متناسب با تعداد ماشین های خود تقسیم کنید.
در هر یک از ماشین ها
$
\sum _ {i = p} ^ q (h_\theta (x ^{(i)}) - y ^{(i)})
. x ^{(i)} _ j
$
را محاسبه کنید.

کاهش نگاشت یا همان MapReduce تمام این نگاشت ها را گرفته و با محاسبه آن ها را کاهش می‌دهد:

$$
\Theta _ j := \Theta _ j - \alpha (temp ^ {(1)} _ j + temp ^ {(2)} _ j + ... + temp ^ {(z)} _ j)
$$

برای همه $j = 0, ..., n$ ها.

این صرفاً گرفتن هزینه محاسبه شده از همه ماشین ها ، محاسبه میانگین آنها، ضرب در نرخ یادگیری و به روزرسانی تتا است.


الگوریتم یادگیری شما MapReduceable است اگر بتوان آن را به صورت مجموعی از توابع بر روی مجموعه آموزش محاسبه کرد. رگرسیون خطی و رگرسیون لجستیک به راحتی قابل موازی شدن هستند.
